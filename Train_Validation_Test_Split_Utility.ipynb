{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TRZXa9BRW6qT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfe9015-d049-44a6-9fd3-45b962ba473f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1769 files belonging to 2 classes.\n",
            "Using 1239 files for training.\n",
            "Found 1769 files belonging to 2 classes.\n",
            "Using 530 files for validation.\n",
            "Train batches: 39\n",
            "Validation batches: 11\n",
            "Test batches: 6\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# ✅ Use your local Google Drive path (no need to download zip)\n",
        "data_dir = '/content/drive/MyDrive/cats_and_dogs_filtered/train'\n",
        "\n",
        "# ✅ Function to split dataset\n",
        "def create_split_datasets(data_dir, img_size=(160, 160), batch_size=32, val_split=0.2, test_split=0.1):\n",
        "    full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        image_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=val_split + test_split,\n",
        "        subset=\"training\",\n",
        "        seed=123\n",
        "    )\n",
        "\n",
        "    valtest_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        image_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=val_split + test_split,\n",
        "        subset=\"validation\",\n",
        "        seed=123\n",
        "    )\n",
        "\n",
        "    val_batches = int(val_split / (val_split + test_split) * len(valtest_dataset))\n",
        "\n",
        "    val_dataset = valtest_dataset.take(val_batches)\n",
        "    test_dataset = valtest_dataset.skip(val_batches)\n",
        "\n",
        "    return full_dataset, val_dataset, test_dataset\n",
        "\n",
        "# ✅ Create datasets from the unzipped local directory\n",
        "train_ds, val_ds, test_ds = create_split_datasets(data_dir)\n",
        "\n",
        "# ✅ Print dataset sizes\n",
        "print(f\"Train batches: {len(train_ds)}\")\n",
        "print(f\"Validation batches: {len(val_ds)}\")\n",
        "print(f\"Test batches: {len(test_ds)}\")\n"
      ]
    }
  ]
}